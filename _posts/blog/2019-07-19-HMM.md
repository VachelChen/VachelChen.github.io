---
layout: post
title: 隐马尔可夫模型的理解以及三个问题的解决
categories: HMM
description: 初次接触语音识别领域，第一大关就是隐马尔科夫模型，作为绝对的小白，这里记录了一些学习体会。
keywords: HMM
---

隐形马尔可夫模型，英文是 Hidden Markov Models，所以以下就简称 HMM。
既是马尔可夫模型，就一定存在马尔可夫链，该马尔可夫链服从马尔可夫性质：即无记忆性。也就是说，这一时刻的状态，受且只受前一时刻的影响，而不受更往前时刻的状态的影响。

先来看一个类似例子：

![HMMeg](/images/blog/HMMeg.png)

在这个马尔可夫模型中，存在三个状态，Sunny， Rainy， Cloudy，同时图片上标的是各个状态间的转移概率（图中箭头的权）。

这里我们主要先来了解一下其中的Elements。

###### 1.N(Number of states N)
这里的N表示状态数，放在上述例子就是Sunny， Rainy， Cloudy，而这些状态是隐式的、不可观测的（我们可以假想一个犯人在没有窗户的监狱里，他只能根据守卫的穿着猜测天气）。同时规定：t时刻的状态是qt。

###### 2.M(Model parameter M)
模型参数M是我们观测序列的数量，由上例子：

如果规定

```yaml
Sunny ————> 守卫穿短袖的概率大
Rainy ————> 守卫带伞的概率大
Cloudy————> 守卫穿棉衣的概率大
```
那么犯人所观测的守卫穿短袖、守卫带伞、守卫穿棉衣数量就是M。

###### 3.π (Initial state distribution π)
初始状态分布 π ，即我们考虑状态转移前最初的状态，也就是以上例子中的第一天会是什么天气。

![pai](/images/blog/pai.png)


###### 4.A (State transition probability distribution)
表示状态的转移可能分布。

![A](/images/blog/A.png)

表示t时刻状态为i，在t+1时刻状态为j的概率，也就是上例中比如第一天下雨第二天会是晴天的概率。

###### 5.B (Observation symbol probability distribution)
表示可观测信号可能分布。

###### λ=(A,B,π)
λ 知道则代表三个概率知道，λ 用于构造出模型。

![B](/images/blog/B.png)![bj](/images/blog/bj.png)


有了基础的认识结合实际例子来理解三个问题会好一点：
在这里，为了简化，把天气情况简单归结为晴天和雨天两种情况。雨天，她选择去散步，购物，收拾的概率分别是0.1，0.4，0.5， 而如果是晴天，她选择去散步，购物，收拾的概率分别是0.6，0.3，0.1。而天气的转换情况如下：这一天下雨，则下一天依然下雨的概率是0.7，而转换成晴天的概率是0.3；这一天是晴天，则下一天依然是晴天的概率是0.6，而转换成雨天的概率是0.4. 同时还存在一个初始概率，也就是第一天下雨的概率是0.6， 晴天的概率是0.4.

![HMMegeg](/images/blog/HMMegeg.png)


根据以上的信息，我们得到了 HMM的一些基本要素：初始概率分布 π，状态转移矩阵 A，观测量的概率分布 B，同时有两个状态，三种可能的观测值。

现在，重点是要了解并解决HMM 的三个问题。

* 问题1，已知整个模型，我女朋友告诉我，连续三天，她下班后做的事情分别是：散步，购物，收拾。那么，根据模型，计算产生这些行为的概率是多少。

* 问题2，同样知晓这个模型，同样是这三件事，我女朋友要我猜，这三天她下班后北京的天气是怎么样的。这三天怎么样的天气才最有可能让她做这样的事情。

* 问题3，最复杂的，我女朋友只告诉我这三天她分别做了这三件事，而其他什么信息我都没有。她要我建立一个模型，晴雨转换概率，第一天天气情况的概率分布，根据天气情况她选择做某事的概率分布。

<table><tr><td bgcolor=yellow>总结隐马尔可夫模型有3个问题：概率计算问题、学习问题、预测问题</td></tr></table>

## Three Basic Problems
#### Solution to Problem 1
###### The Forward Algorithm

概率计算问题描述如下：

已知一个隐马尔可夫模型λ=（A，B，λ），已知一个观测序列O=（o1, o2,……，oT）

问题：请计算在模型λ下观测序列O出现的概率P(O&#124; λ)

新变量定义如下：
![TFA](/images/blog/TFA.png)

![TFAdo](/images/blog/TFAdo.jpg)

有关第一个问题的解决可以使用向前算法。
先计算 t=1时刻，发生『散步』一行为的概率，如果下雨，则为 P(散步，下雨)=P（第一天下雨）X P（散步 &#124; 下雨）=0.6X0.1=0.06；晴天，P（散步，晴天）=0.4X0.6=0.24

t=2 时刻，发生『购物』的概率，当然，这个概率可以从 t=1 时刻计算而来。

如果t=2下雨，则 P（第一天散步，第二天购物， 第二天下雨）= 【P（第一天散步，第一天下雨）X P（第二天下雨 &#124; 第一天下雨）+P（第一天散步，第一天晴天）X P(第二天下雨 &#124; 第一天晴天)】X P（第二天购物 &#124; 第二天下雨）=【0.06X0.7+0.24X0.3】X0.4=0.0552

如果 t=2晴天，则 P（第一天散步，第二天购物，第二天晴天）=0.0486 （同理可得，请自行推理）

如果 t=3，下雨，则 P（第一天散步，第二天购物，第三天收拾，第三天下雨）=【P（第一天散步，第二天购物，第二天下雨）X P（第三天下雨 &#124; 第二天下雨）+ P（第一天散步，第二天购物，第二天天晴）X P（第三天下雨 &#124; 第二天天晴）】X P（第三天收拾 &#124; 第三天下雨）=【0.0552X0.7+0.0486X0.4】X0.5= 0.02904

如果t=3，晴天，则 P（第一天散步，第二天购物，第三天收拾，第三天晴天）= 0.004572

那么 P（第一天散步，第二天购物，第三天收拾），这一概率则是第三天，下雨和晴天两种情况的概率和。0.02904+0.004572=0.033612.

引用摘自：[https://www.zhihu.com/question/20962240/answer/64187492](https://www.zhihu.com/question/20962240/answer/64187492)

###### The Backward Algorithm

后向算法与前向算法大致相同，已知隐马尔可夫模型λ=（A, B, π），定义到时刻t(t<T)状态为q&#60;i&#62;的条件下，从t+1到T的部分观测序列为o&#60;t+1&#62;,o&#60;t+2&#62;，...,O<T>的概率为后向概率，记作：
![TBA](/images/blog/TBA.png)

![TBAdo](/images/blog/TBAdo.jpg)

核心是通过模型λ=(A, B, π)以及时刻t+1的β&#60;t+1&#62;(i)来递推得到前一个时刻t的β&#60;t&#62;(i).直到递推得到β<0>(i)

###### Scaling the Forward and Backward Variables

概率之间会出现越乘越小的问题，若最后不能被计算机所识别则前面一切的累计都是徒劳的，所以进行归一化处理十分有必要，具体推导如下：

(1)首先进行归一化处理：
![scaling1](/images/blog/scaling1.png)

(1)首先进行归一化处理：
![scaling1](/images/blog/scaling1.png)

#### Solution to Problem 2