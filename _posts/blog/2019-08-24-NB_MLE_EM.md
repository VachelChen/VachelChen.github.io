---
layout: post
title: 朴素贝叶斯分类法、最大似然估计和EM算法
categories: theory
description: 回家两星期后的第一堂课就是纯理论的课，上课基本感觉在梦游，现在回寝后终于有自己的时间用来自己看一看。
keywords: NB,EM
---

朴素贝叶斯、最大似然估计和EM算法的作用在于对HMM中的B也就是发射概率进行求解，当时提到B的求解与高斯混合分布的两个参数有关， miu 和 sigema方 前者在于对多维求均值，后者用到协方差矩阵，这三个算法应该对我们的求解有帮助。

## 一、Naive Bayes Model

	P(A|B)=P(B|A)*P(A)/P(B) 
	其中：
	P(A)：先验概率，表示每种类别的分布的概率。
	P(B|A)：类条件概率，表示在某种类别前提下，某事发生的概率。
	P(A|B)：后验概率，表示某事发生了，并且它属于某一类别的概率，
	有了这个后验概率，我们就可以对样本进行分类。后验概率越大，
	说明某事物属于这个类别的可能性越大，我们越有理由把它归到这个类别下。

说到贝叶斯在一开始的第一反应是这个公式，因为在求条件概率时会遇到后三个条件已知所以可以用来转换，但何又为朴素贝叶斯呢？

朴素贝叶斯适用于分类，分类从数学角度有：类别集合、特征集合的定义，分类算法的任务就是构造分类器。

![by](/images/blog/by.png)

我们最终求的p(类别&#124;特征)即可！就相当于完成了我们的任务。

###### 贝叶斯分类举例

给定数据如下：

![bydata](/images/blog/bydata.png)

现在给我们的问题是，如果一对男女朋友，男生想女生求婚，男生的四个特点分别是不帅，性格不好，身高矮，不上进，请你判断一下女生是嫁还是不嫁？
 
这是一个典型的分类问题，转为数学问题就是比较p(嫁&#124;(不帅、性格不好、身高矮、不上进))与p(不嫁&#124;(不帅、性格不好、身高矮、不上进))的概率，谁的概率大，我就能给出嫁或者不嫁的答案！

这里我们联系到朴素贝叶斯公式：

![bygs](/images/blog/bygs.png)

我们需要求p(嫁&#124;(不帅、性格不好、身高矮、不上进)),这是我们不知道的，但是通过朴素贝叶斯公式可以转化为好求的三个量.
 
p(不帅、性格不好、身高矮、不上进&#124;嫁)、p（不帅、性格不好、身高矮、不上进)、p(嫁)又如何得来呢？这就是贝叶斯算法里最简单又最重要的朴素贝叶斯发挥作用的时候了，我们假设长相、性格、身高和三观相互独立，那么就可以得到：

	p(不帅、性格不好、身高矮、不上进|嫁) = p(不帅|嫁)*p(性格不好|嫁)*p(身高矮|嫁)*p(不上进|嫁)

假设特征之间相互独立有两个原因：

1. 如果不假设独立，运算十分困难，这个例子四个特征就是四维空间，但比如MFCC的39维，每个维度取值也多，那么通过统计来估计后面的值几乎不可做。

2. 如果不假设独立，我们就需要找到满足现有情况的所有个数，由于数据的稀疏性，很容易统计到0。

![tjgldlx](/images/blog/tjgldlx.png)

其中的p如何计算？

	从我们的训练数据中，例如P（嫁），就是找出嫁的样本，看看占样本总数的多少。

比如这个例子中就是6/12=1/6。

其余的算法也是如此，都是要基于已有的训练集。

这就是有关朴素贝叶斯的算法的分类。

---

## 二、Maximum-Likelihood Estimation

这里首先阐明根据查找的资料，Maximum-Likelihood Estimation有的翻译叫极大似然估计有的翻译叫最大似然估计，其实两者是一样的。
说到这里我发现其实我对这个概念并不陌生，之所以没想起来，是因为当时在概率课上学习极大似然估计的时候完全没有理解他的用法，只知道在什么样的题型中来使用，套用固定的四步算法来拿到分数。

重新理解极大似然估计，用一张图来说明：

![MLE](/images/blog/MLE.png)

 总结起来，最大似然估计的目的就是：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。所以说，概率是根据条件推测结果，而似然则是根据结果反推条件

 原理：极大似然估计是建立在极大似然原理的基础上的一个统计方法，是概率论在统计学中的应用。极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。

刚刚的问题在朴素贝叶斯的帮助下除了精读有所下降，似乎顺理成章的解决了，其实在实际问题中很难这么幸运，比如我们刚才在完成朴素贝叶斯变换后所计算P，就是在刚刚给的表格中完成一个数数的操作，也就是说我们能获得的数据可能只有有限数目的样本数据，而

先验概率P（A）     			| 嫁还是不嫁的概率 
类条件分布P（B&#124;A）  在嫁/不嫁的条件下，某特征分布的概率

都是未知的根据仅有的样本数据进行分类时，一种可行的办法是我们需要先对先验概率和类条件概率进行估计，然后再套用贝叶斯分类器。

先验概率的估计较简单：

1、每个样本所属的自然状态都是已知的（有监督学习）；
2、依靠经验；
3、用训练样本中各类出现的频率估计。


类条件概率的估计（非常难），原因包括：
概率密度函数包含了一个随机变量的全部信息；样本数据可能不多；特征向量x的维度可能很大等等。
总之要直接估计类条件概率的密度函数很难。解决的办法就是，把估计完全未知的概率密度 P(B &#124;A) 转化为估计参数。这里就将概率密度估计问题转化为参数估计问题，极大似然估计就是一种参数估计方法。

	而求极大似然函数估计值的一般步骤如下：
	（1） 写出似然函数；
	（2） 对似然函数取对数，并整理；
	（3） 求导数 ；
	（4） 解似然方程 。

###### 最大似然估计举例

假定我们需要统计七月在线10万学员中男生女生的身高分布，怎么统计呢？考虑到10万的数量巨大，所以不可能一个一个的去统计。对的，随机抽样，从10万学员中随机抽取100个男生，100个女生，然后依次统计他们各自的身高。

    对于这个问题，我们通过数学建模抽象整理下

首先我们从10万学员中抽取到100个男生/女生的身高，组成样本集X，X={x1,x2,…,xN}，其中xi表示抽到的第i个人的身高，N等于100，表示抽到的样本个数。
假定男生的身高服从正态分布，女生的身高则服从另一个正态分布。
但是这两个分布的均值u和方差∂2都不知道，设为未知参数θ=[u, ∂]T
现在需要用极大似然法（MLE），通过这100个男生或100个女生的身高结果，即样本集X来估计两个正态分布的未知参数θ，问题定义相当于已知X，求θ，换言之就是求p(θ &#124;x)因为这些男生（的身高）是服从同一个高斯分布p(x &#124;θ)的。那么抽到男生A（的身高）的概率是p(xA &#124;θ)，抽到男生B的概率是p(xB &#124;θ)，考虑到他们是独立的，所以同时抽到男生A和男生B的概率是p(xA &#124;θ)* p(xB &#124;θ)。

同理，我从分布是p(x &#124;θ)的总体样本中同时抽到这100个男生样本的概率，也就是样本集X中100个样本的联合概率（即它们各自概率的乘积），用下式表示：

![MLEJS](/images/blog/MLEJS.png)
插一句，有个文章中会用这个表示p(x &#124; θ)，有的文章会用p(x;θ)，不过，不管用哪种表示方法，本质都是一样的。当然，如果涉及到Bayes公式的话，用前者表示p(x &#124; θ)更好。

在七月在线那么多男学员中，我一抽就抽到这100个男生，而不是其他人，那说明在整个学校中，这100个人（的身高）出现的概率最大啊，这个概率就是上面这个似然函数L(θ)，怎么做到的呢？换言之，**怎样的θ能让L(θ)最大？**

假定我们找到一个参数![sita](/images/blog/sita.png)，能使似然函数L(θ)最大（也就是说抽到这100个男生的身高概率最大），则![sita](/images/blog/sita.png)应该是“最可能”的参数值，相当于θ的极大似然估计量。记为：

![argmax](/images/blog/argmax.png)

这里的L(θ)是连乘的，为了便于分析，我们可以定义对数似然函数，将其变成连加的：

![logsigema](/images/blog/logsigema.png)

现在需要使θ的似然函数L(θ)极大化，然后极大值对应的θ就是我们的估计。

## 三、Expectation-Maximization algorithm

EM出现的原因就是抽取的样本不知道是哪个分布抽取的。例如之前的最大似然所说的，现在如果男女两种高斯分布的人混在一块了，我们又不知道哪些人属于第一个高斯分布，哪些属于第二个，所以就没法估计这两个分布的参数。反过来，只有当我们对这两个分布的参数作出了准确的估计的时候，才能知道到底哪些人属于第一个分布，那些人属于第二个分布。**所以这里就是说EM估计就是因为多了一个隐含变量（抽取得到的每个样本都不知道是从哪个分布抽取的）使得本来简单的可以求解的问题变复杂了**。

一般的用Y表示观测到的随机变量的数据，Z表示隐随机变量的数据(因为我们观测不到结果是从哪个概率分布中得出的，所以将这个叫做隐变量)。于是Y和Z连在一起被称为完全数据，仅Y一个被称为不完全数据。

这时有没有发现EM算法面临的问题主要就是：有个隐变量数据Z。而如果Z已知的话，那问题就可用极大似然估计求解了。 于是乎，怎么把Z变成已知的？

###### EM算法举例一

假定你是一名厨师，现在需要把锅里的菜平均分配到两个碟子里。如果只有一个碟子乘菜那就什么都不用说了，但问题是有2个碟子，正因为根本无法估计一个碟子里应该乘多少菜，所以无法一次性把菜完全平均分配。

解法：
大厨先把锅里的菜一股脑倒进两个碟子里，然后看看哪个碟子里的菜多，就把这个碟子中的菜往另一个碟子中匀匀，之后重复多次匀匀的过程，直到两个碟子中菜的量大致一样。 上面的例子中，**平均分配这个结果是“观测数据z”，为实现平均分配而给每个盘子分配多少菜是“待求参数θ”，分配菜的手感就是“概率分布”。**
于是若只有一个盘子，那概率分布就确定了（“把锅里的菜全部倒到一个盘子”这样的手感是个人都有吧），而因为有两个盘子，所以“给一个盘子到多少菜才好”的手感就有些模糊不定，不过我们可以采用上面的解法来实现最终目标。

EM算法的思想就是：

	1. 给θ自主规定个初值（既然我不知道想实现“两个碟子平均分配锅里的菜”的话每个碟子需要有多少菜，那我就先估计个值）；
	2. 根据给定观测数据和当前的参数θ，求未观测数据z的条件概率分布的期望（在上一步中，已经根据手感将菜倒进了两个碟子，然后这一步根据“两个碟子里都有菜”和“当前两个碟子都有多少菜”来判断自己倒菜的手感）；
	3. 上一步中z已经求出来了，于是根据极大似然估计求最优的θ’（手感已经有了，那就根据手感判断下盘子里应该有多少菜，然后把菜匀匀）；
	4. 因为第二步和第三步的结果可能不是最优的，所以重复第二步和第三步，直到收敛（重复多次匀匀的过程，直到两个碟子中菜的量大致一样）。

而上面的第二步被称作E步（求期望），第三步被称作M步（求极大化），从而不断的E、M。

###### EM算法举例二

两枚硬币A和B，假定随机抛掷后正面朝上概率分别为PA，PB。为了估计这两个硬币朝上的概率，咱们轮流抛硬币A和B，每一轮都连续抛5次，总共5轮：

硬币|结果|统计
A	|正正反正反|	3正-2反
B	|反反正正反|	2正-3反
A	|正反反反反|	1正-4反
B	|正反反正正|	3正-2反
A	|反正正反反|	2正-3反
硬币A被抛了15次，在第一轮、第三轮、第五轮分别出现了3次正、1次正、2次正，所以很容易估计出PA，类似的，PB也很容易计算出来，如下：

PA = （3+1+2）/ 15 = 0.4
PB= （2+3）/10 = 0.5

问题来了，如果我们不知道抛的硬币是A还是B呢（即硬币种类是隐变量），然后再轮流抛五轮，得到如下结果：

硬币	|结果|统计
Unknown	|正正反正反|	3正-2反
Unknown	|反反正正反|	2正-3反
Unknown	|正反反反反|	1正-4反
Unknown	|正反反正正|	3正-2反
Unknown	|反正正反反|	2正-3反
OK，问题变得有意思了。现在我们的目标没变，还是估计PA和PB，需要怎么做呢？

显然，此时我们多了一个硬币种类的隐变量，设为z，可以把它认为是一个5维的向量（z1,z2,z3,z4,z5)，代表每次投掷时所使用的硬币，比如z1，就代表第一轮投掷时使用的硬币是A还是B。

但是，这个变量z不知道，就无法去估计PA和PB，所以，我们必须先估计出z，然后才能进一步估计PA和PB。
可要估计z，我们又得知道PA和PB，这样我们才能用极大似然概率法则去估计z，这不是鸡生蛋和蛋生鸡的问题吗，如何破？
答案就是先随机初始化一个PA和PB，用它来估计z，然后基于z，还是按照最大似然概率法则去估计新的PA和PB，如果新的PA和PB和我们初始化的PA和PB一样，请问这说明了什么？

这说明我们初始化的PA和PB是一个相当靠谱的估计！

就是说，我们初始化的PA和PB，按照最大似然概率就可以估计出z，然后基于z，按照最大似然概率可以反过来估计出P1和P2，当与我们初始化的PA和PB一样时，说明是P1和P2很有可能就是真实的值。这里面包含了两个交互的最大似然估计。

如果新估计出来的PA和PB和我们初始化的值差别很大，怎么办呢？就是继续用新的P1和P2迭代，直至收敛。

我们不妨这样，先随便给PA和PB赋一个值，比如：
硬币A正面朝上的概率PA = 0.2
硬币B正面朝上的概率PB = 0.7

然后，我们看看第一轮抛掷最可能是哪个硬币。
如果是硬币A，得出3正2反的概率为 0.2*0.2*0.2*0.8*0.8 = 0.00512
如果是硬币B，得出3正2反的概率为0.7*0.7*0.7*0.3*0.3=0.03087
然后依次求出其他4轮中的相应概率。做成表格如下（标粗表示其概率更大）：

轮数	|若是硬币A|若是硬币B
1	|0.00512，即0.2 0.2 0.2 0.8 0.8，3正-2反|	0.03087，3正-2反
2	|0.02048，即0.2 0.2 0.8 0.8 0.8，2正-3反|	0.01323，2正-3反
3	|0.08192，即0.2 0.8 0.8 0.8 0.8，1正-4反|	0.00567，1正-4反
4	|0.00512，即0.2 0.2 0.2 0.8 0.8，3正-2反|	0.03087，3正-2反
5	|0.02048，即0.2 0.2 0.8 0.8 0.8，2正-3反|	0.01323，2正-3反
按照最大似然法则：
第1轮中最有可能的是**硬币B**
第2轮中最有可能的是**硬币A**
第3轮中最有可能的是**硬币A**
第4轮中最有可能的是**硬币B**
第5轮中最有可能的是**硬币A**

我们就把概率更大，即更可能是A的，即第2轮、第3轮、第5轮出现正的次数2、1、2相加，除以A被抛的总次数15（A抛了三轮，每轮5次），作为z的估计值，B的计算方法类似。然后我们便可以按照最大似然概率法则来估计新的PA和PB。

PA = （2+1+2）/15 = 0.33
PB =（3+3）/10 = 0.6

设想我们是全知的神，知道每轮抛掷时的硬币就是如本文本节开头标示的那样，那么，PA和PB的最大似然估计就是0.4和0.5（下文中将这两个值称为PA和PB的真实值）。那么对比下我们初始化的PA和PB和新估计出的PA和PB：

初始化的PA	|计出的PA|	真实的PA	|初始化的PB|	估计出的PB|	真实的PB
0.2	|0.33|	0.4|	0.7|	0.6|	0.5
看到没？我们估计的PA和PB相比于它们的初始值，更接近它们的真实值了！就这样，不断迭代 不断接近真实值，这就是EM算法的奇妙之处。

可以期待，我们继续按照上面的思路，用估计出的PA和PB再来估计z，再用z来估计新的PA和PB，反复迭代下去，就可以最终得到PA = 0.4，PB=0.5，此时无论怎样迭代，PA和PB的值都会保持0.4和0.5不变，于是乎，我们就找到了PA和PB的最大似然估计。



最大期望算法（Expectation-maximization algorithm，又译为期望最大化算法），是在概率模型中寻找参数最大似然估计或者最大后验估计的算法，其中概率模型依赖于无法观测的隐性变量。

	最大期望算法经过两个步骤交替进行计算：
	第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值；
	第二步是最大化（M），最大化在E步上求得的最大似然值来计算参数的值。M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。
          

























