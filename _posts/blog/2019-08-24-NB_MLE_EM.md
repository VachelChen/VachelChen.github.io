---
layout: post
title: 朴素贝叶斯分类法、最大似然估计和EM算法
categories: theory
description: 回家两星期后的第一堂课就是纯理论的课，上课基本感觉在梦游，现在回寝后终于有自己的时间用来自己看一看。
keywords: NB,EM
---

朴素贝叶斯、最大似然估计和EM算法的作用在于对HMM中的B也就是发射概率进行求解，当时提到B的求解与高斯混合分布的两个参数有关， miu 和 sigema方 前者在于对多维求均值，后者用到协方差矩阵，这三个算法应该对我们的求解有帮助。

## 一、Naive Bayes Model

	P(A|B)=P(B|A)*P(A)/P(B) 
	其中：
	P(A)：先验概率，表示每种类别的分布的概率。
	P(B|A)：类条件概率，表示在某种类别前提下，某事发生的概率。
	P(A|B)：后验概率，表示某事发生了，并且它属于某一类别的概率，
	有了这个后验概率，我们就可以对样本进行分类。后验概率越大，
	说明某事物属于这个类别的可能性越大，我们越有理由把它归到这个类别下。

说到贝叶斯在一开始的第一反应是这个公式，因为在求条件概率时会遇到后三个条件已知所以可以用来转换，但何又为朴素贝叶斯呢？

朴素贝叶斯适用于分类，分类从数学角度有：类别集合、特征集合的定义，分类算法的任务就是构造分类器。

![by](/images/blog/by.png)

我们最终求的p(类别&#124;特征)即可！就相当于完成了我们的任务。

接下来引入一个网络上我觉得十分亲切的例子：（老师上课说的图书分类的例子讲的我有点晕）

给定数据如下：

![bydata](/images/blog/bydata.png)

现在给我们的问题是，如果一对男女朋友，男生想女生求婚，男生的四个特点分别是不帅，性格不好，身高矮，不上进，请你判断一下女生是嫁还是不嫁？
 
这是一个典型的分类问题，转为数学问题就是比较p(嫁&#124;(不帅、性格不好、身高矮、不上进))与p(不嫁&#124;(不帅、性格不好、身高矮、不上进))的概率，谁的概率大，我就能给出嫁或者不嫁的答案！

这里我们联系到朴素贝叶斯公式：

![bygs](/images/blog/bygs.png)

我们需要求p(嫁&#124;(不帅、性格不好、身高矮、不上进)),这是我们不知道的，但是通过朴素贝叶斯公式可以转化为好求的三个量.
 
p(不帅、性格不好、身高矮、不上进&#124;嫁)、p（不帅、性格不好、身高矮、不上进)、p(嫁)又如何得来呢？这就是贝叶斯算法里最简单又最重要的朴素贝叶斯发挥作用的时候了，我们假设长相、性格、身高和三观相互独立，那么就可以得到：

	p(不帅、性格不好、身高矮、不上进|嫁) = p(不帅|嫁)*p(性格不好|嫁)*p(身高矮|嫁)*p(不上进|嫁)

假设特征之间相互独立有两个原因：

1. 如果不假设独立，运算十分困难，这个例子四个特征就是四维空间，但比如MFCC的39维，每个维度取值也多，那么通过统计来估计后面的值几乎不可做。

2. 如果不假设独立，我们就需要找到满足现有情况的所有个数，由于数据的稀疏性，很容易统计到0。

![tjgldlx](/images/blog/tjgldlx.png)

其中的p如何计算？

	从我们的训练数据中，例如P（嫁），就是找出嫁的样本，看看占样本总数的多少。

比如这个例子中就是6/12=1/6。

其余的算法也是如此，都是要基于已有的训练集。

这就是有关朴素贝叶斯的算法的分类。

---

## 二、Maximum-Likelihood Estimation

这里首先阐明根据查找的资料，Maximum-Likelihood Estimation有的翻译叫极大似然估计有的翻译叫最大似然估计，其实两者是一样的。
说到这里我发现其实我对这个概念并不陌生，之所以没想起来，是因为当时在概率课上学习极大似然估计的时候完全没有理解他的用法，只知道在什么样的题型中来使用，套用固定的四步算法来拿到分数。

重新理解极大似然估计，用一张图来说明：

![MLE](/images/blog/MLE.png)

 总结起来，最大似然估计的目的就是：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。

 原理：极大似然估计是建立在极大似然原理的基础上的一个统计方法，是概率论在统计学中的应用。极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。

刚刚的问题在朴素贝叶斯的帮助下除了精读有所下降，似乎顺理成章的解决了，其实在实际问题中很难这么幸运，比如我们刚才在完成朴素贝叶斯变换后所计算P，就是在刚刚给的表格中完成一个数数的操作，也就是说我们能获得的数据可能只有有限数目的样本数据，而

先验概率P（A）     &#124; 嫁还是不嫁的概率 
类条件分布P（B|A） &#124; 在嫁/不嫁的条件下，某特征分布的概率

都是未知的根据仅有的样本数据进行分类时，一种可行的办法是我们需要先对先验概率和类条件概率进行估计，然后再套用贝叶斯分类器。

先验概率的估计较简单：
1、每个样本所属的自然状态都是已知的（有监督学习）；
2、依靠经验；
3、用训练样本中各类出现的频率估计。

类条件概率的估计（非常难），原因包括：概率密度函数包含了一个随机变量的全部信息；样本数据可能不多；特征向量x的维度可能很大等等。总之要直接估计类条件概率的密度函数很难。解决的办法就是，把估计完全未知的概率密度 P(B|A) 转化为估计参数。这里就将概率密度估计问题转化为参数估计问题，极大似然估计就是一种参数估计方法。

	而求极大似然函数估计值的一般步骤如下：
	（1） 写出似然函数；
	（2） 对似然函数取对数，并整理；
	（3） 求导数 ；
	（4） 解似然方程 。

这里需要注意的是，这里的参数只是对应了一个类别，比如说我想知道东北的女生嫁和不嫁的情况，分为辽宁、吉林和黑龙江。如果两个类别混在一起，那么就是下面的EM估计了。 

## 三、Expectation-Maximization algorithm

EM出现的原因就是抽取的样本不知道是哪个分布抽取的。例如刚开始的最大似然所说的，但现在两种高斯分布的人混在一块了，我们又不知道哪些人属于第一个高斯分布，哪些属于第二个，所以就没法估计这两个分布的参数。反过来，只有当我们对这两个分布的参数作出了准确的估计的时候，才能知道到底哪些人属于第一个分布，那些人属于第二个分布。***所以这里就是说EM估计就是因为多了一个隐含变量（抽取得到的每个样本都不知道是从哪个分布抽取的）使得本来简单的可以求解的问题变复杂了***。